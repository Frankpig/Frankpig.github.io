<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://frankpig.github.io</id>
    <title>大帅哥的博客哟</title>
    <updated>2022-10-14T07:00:12.585Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://frankpig.github.io"/>
    <link rel="self" href="https://frankpig.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://frankpig.github.io/images/avatar.png</logo>
    <icon>https://frankpig.github.io/favicon.ico</icon>
    <rights>All rights reserved 2022, 大帅哥的博客哟</rights>
    <entry>
        <title type="html"><![CDATA[k8快速运行nginx]]></title>
        <id>https://frankpig.github.io/post/k8-kuai-su-yun-xing-nginx/</id>
        <link href="https://frankpig.github.io/post/k8-kuai-su-yun-xing-nginx/">
        </link>
        <updated>2022-10-14T05:50:56.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>为了方便配置，创建三个文件。
<ol>
<li><img src="https://frankpig.github.io/post-images/1665726986727.png" alt="" loading="lazy"></li>
<li>如果单纯为了快速部署，configmap可以不要。</li>
</ol>
</li>
</ul>
<ol>
<li>nginx-deployment.yaml</li>
</ol>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx 
    spec:
      containers:
      - name: nginx 
        image: nginx 
        ports:
        - containerPort: 80
        volumeMounts:
          - name: nginx-config
            mountPath: /etc/nginx/nginx.conf
            subPath: nginx.conf
      volumes:
        - name: nginx-config
          configMap:
            name: nginx-config
            items:
            - key: nginx.conf
              path: nginx.conf

</code></pre>
<ol start="2">
<li>nginx-service.yaml</li>
</ol>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: nginx
spec:
  ports:
  - port: 88
    targetPort: 80
  selector:
    app: nginx
  type: NodePort
</code></pre>
<ol start="3">
<li>nginx-configmap.yaml</li>
</ol>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
    name: nginx-config
data:
  nginx.conf: |
    user  nginx;
    worker_processes  auto;

    error_log  /var/log/nginx/error.log notice;
    pid        /var/run/nginx.pid;


    events {
        worker_connections  1024;
    }


    http {
        include       /etc/nginx/mime.types;
        default_type  application/octet-stream;

        log_format  main  '$remote_addr - $remote_user [$time_local] &quot;$request&quot; '
                          '$status $body_bytes_sent &quot;$http_referer&quot; '
                          '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;';

        access_log  /var/log/nginx/access.log  main;

        sendfile        on;
        #tcp_nopush     on;

        keepalive_timeout  65;

        #gzip  on;

        include /etc/nginx/conf.d/*.conf;
    }

</code></pre>
<ul>
<li>
<p>创建完文件之后，执行以下命令</p>
<ul>
<li><code>kubectl apply -f nginx-deployment.yaml</code></li>
<li><code>kubectl apply -f nginx-configmap.yaml</code></li>
<li><code>kubectl apply -f nginx-service.yaml</code></li>
</ul>
</li>
<li>
<p>查看运行状态</p>
<ul>
<li><img src="https://frankpig.github.io/post-images/1665727974370.png" alt="" loading="lazy"></li>
<li><img src="https://frankpig.github.io/post-images/1665728041940.png" alt="" loading="lazy"></li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kind简易使用指南]]></title>
        <id>https://frankpig.github.io/post/kind-jian-yi-shi-yong-zhi-nan/</id>
        <link href="https://frankpig.github.io/post/kind-jian-yi-shi-yong-zhi-nan/">
        </link>
        <updated>2022-10-14T02:54:55.000Z</updated>
        <content type="html"><![CDATA[<ol>
<li>安装kind
<ol>
<li>brew install kind</li>
<li><img src="https://frankpig.github.io/post-images/1665717647283.png" alt="" loading="lazy"></li>
</ol>
</li>
<li>查看kind
<ol>
<li><img src="https://frankpig.github.io/post-images/1665717582633.png" alt="" loading="lazy"></li>
</ol>
</li>
<li>首先运行电脑上的docker server/docker desktop。</li>
<li>用kind快速创建集群
<ol>
<li>kind create cluster</li>
<li>kind create cluster --name dashuaige</li>
</ol>
</li>
<li>删除集群
<ol>
<li>kind delete cluster <cluster-name></li>
</ol>
</li>
<li>查看集群
<ol>
<li>kind get clusters</li>
<li><img src="https://frankpig.github.io/post-images/1665717816372.png" alt="" loading="lazy"></li>
</ol>
</li>
<li>创建自定义集群
<ol>
<li>kind create cluster --config kind-example-config.yaml</li>
<li></li>
</ol>
<pre><code class="language-yaml">
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
- role: worker
- role: worker
</code></pre>
</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[底层 Linux 容器运行时之发展史 ]]></title>
        <id>https://frankpig.github.io/post/di-ceng-linux-rong-qi-yun-xing-shi-zhi-fa-zhan-shi/</id>
        <link href="https://frankpig.github.io/post/di-ceng-linux-rong-qi-yun-xing-shi-zhi-fa-zhan-shi/">
        </link>
        <updated>2022-09-16T07:59:55.000Z</updated>
        <content type="html"><![CDATA[<p>转载自: https://zhuanlan.zhihu.com/p/38121678</p>
<p>传统的容器是操作系统中的进程，通常具有如下 3 个特性：</p>
<ol>
<li>资源限制<br>
当你在系统中运行多个容器时，你肯定不希望某个容器独占系统资源，所以我们需要使用资源约束来控制 CPU、内存和网络带宽等资源。Linux 内核提供了 cgroup 特性，可以通过配置控制容器进程的资源使用。</li>
<li>安全性配置<br>
一般而言，你不希望你的容器可以攻击其它容器或甚至攻击宿主机系统。容器使用了 Linux 内核的若干特性建立安全隔离，相关特性包括 SELinux、seccomp 和 capabilities。</li>
<li>虚拟隔离<br>
容器外的任何进程对于容器而言都应该不可见。容器应该使用独立的网络。不同的容器对应的进程应该都可以绑定 80 端口。每个容器的内核镜像(image)、根文件系统(rootfs)都应该相互独立。在 Linux 中，我们使用内核的名字空间(namespace)特性提供虚拟隔离(virtual separation)。<br>
那么，具有安全性配置并且在 cgroup 和namespaces下运行的进程都可以称为容器。查看一下centOS操作系统中的 PID 1 的进程 systemd，你会发现 systemd 运行在一个 cgroup 下。</li>
</ol>
<h1 id="tail-1-proc1cgroup">tail -1 /proc/1/cgroup</h1>
<p>1:name=systemd:/</p>
<p>ps 命令让我们看到 systemd 进程具有 SELinux 标签：</p>
<h1 id="ps-ez-grep-systemd">ps -eZ | grep systemd</h1>
<p>system_u:system_r:init_t:s0             1 ?     00:00:48 systemd</p>
<p>以及 capabilities：</p>
<h1 id="grep-cap-proc1status">grep Cap /proc/1/status</h1>
<p>...<br>
CapEff: 0000001fffffffff<br>
CapBnd: 0000001fffffffff<br>
CapBnd:    0000003fffffffff</p>
<p>最后，查看 /proc/1/ns 子目录，你会发现 systemd 运行所在的名字空间。<br>
ls -l /proc/1/ns<br>
lrwxrwxrwx. 1 root root 0 Jan 11 11:46 mnt -&gt; mnt:[4026531840]<br>
lrwxrwxrwx. 1 root root 0 Jan 11 11:46 net -&gt; net:[4026532009]<br>
lrwxrwxrwx. 1 root root 0 Jan 11 11:46 pid -&gt; pid:[4026531836]<br>
...</p>
<p>如果 PID 1 进程（实际上每个系统进程）具有资源约束、安全性配置和名字空间，那么我可以说系统上的每一个进程都运行在容器中。<br>
容器运行时工具也不过是修改了资源约束、安全性配置和名字空间，然后 Linux 内核运行起进程。容器启动后，容器运行时可以在容器内监控 PID 1 进程，也可以监控容器的标准输入/输出，从而进行容器进程的生命周期管理。<br>
容器运行时<br>
你可能自言自语道，“哦，systemd 看起来很像一个容器运行时”。经过若干次关于“为何容器运行时不使用 systemd-nspawn 工具来启动容器”的邮件讨论后，我认为值得讨论一下容器运行时及其发展史。<br>
Docker 通常被称为容器运行时，但“ 容器运行时(container runtime)”是一个被过度使用的词语。当用户提到“容器运行时”，他们其实提到的是为开发人员提供便利的 上层(high-level)工具，包括 Docker，CRI-O 和 RKT。这些工具都是基于 API 的，涉及操作包括从容器仓库拉取容器镜像、配置存储和启动容器等。启动容器通常涉及一个特殊工具，用于配置内核如何运行容器，这类工具也被称为“容器运行时”，下文中我将称其为“底层容器运行时”以作区分。像 Docker、CRI-O 这样的守护进程及形如 Podman、Buildah的命令行工具，似乎更应该被称为“容器管理器”。<br>
早期版本的 Docker 使用 lxc 工具集启动容器，该工具出现在 systemd-nspawn 之前。Red Hat 最初试图将 libvirt （libvirt-lxc）集成到 Docker 中替代 lxc 工具，因为 RHEL 并不支持 lxc。libvirt-lxc 也没有使用 systemd-nspawn，在那时 systemd 团队仅将 systemd-nspawn 视为测试工具，不适用于生产环境。<br>
与此同时，包括我的 Red Hat 团队部分成员在内的 上游(upstream) Docker 开发者，认为应该采用 golang 原生的方式启动容器，而不是调用外部应用。他们的工作促成了 libcontainer 这个 golang 原生库，用于启动容器。Red Hat 工程师更看好该库的发展前景，放弃了 libvirt-lxc。<br>
后来成立 开放容器组织(Open Container Initiative)（OCI）的部分原因就是人们希望用其它方式启动容器。传统的基于namespace隔离的容器已经家喻户晓，但人们也有 虚拟机级别隔离(virtual machine-level isolation)的需求。Intel 和 Hyper.sh 正致力于开发基于 KVM 隔离的容器，Microsoft 致力于开发基于 Windows 的容器。OCI 希望有一份定义容器的标准规范，因而产生了 ref=&quot;https://github.com/opencontainers/runtime-spec&quot;&gt;OCI 运行时规范(Runtime Specification)。<br>
OCI 运行时规范定义了一个 JSON 文件格式，用于描述要运行的二进制，如何容器化以及容器根文件系统的位置。一些工具用于生成符合标准规范的 JSON 文件，另外的工具用于解析 JSON 文件并在该根文件系统（rootfs）上运行容器。Docker 的部分代码被抽取出来构成了 libcontainer 项目，该项目被贡献给 OCI。上游 Docker 工程师及我们自己的工程师创建了一个新的前端工具，用于解析符合 OCI 运行时规范的 JSON 文件，然后与 libcontainer 交互以便启动容器。这个前端工具就是 runc，也被贡献给 OCI。虽然 runc 可以解析 OCI JSON 文件，但用户需要自行生成这些文件。此后，runc 也成为了最流行的底层容器运行时，基本所有的容器管理工具都支持 runc，包括 CRI-O、Docker、Buildah、Podman 和 Cloud Foundry Garden 等。此后，其它工具的实现也参照 OCI 运行时规范，以便可以运行 OCI 兼容的容器。<br>
Clear Containers 和 Hyper.sh 的 runV 工具都是参照 OCI 运行时规范运行基于 KVM 的容器，二者将其各自工作合并到一个名为 Kata 的新项目中。在去年，Oracle 创建了一个示例版本的 OCI 运行时工具，名为 RailCar，使用 Rust 语言编写。但该 GitHub 项目已经两个月没有更新了，故无法判断是否仍在开发。几年前，Vincent Batts 试图创建一个名为 nspawn-oci 的工具，用于解析 OCI 运行时规范文件并启动 systemd-nspawn；但似乎没有引起大家的注意，而且也不是原生的实现。<br>
如果有开发者希望实现一个原生的 systemd-nspawn --oci OCI-SPEC.json 并让 systemd 团队认可和提供支持，那么CRI-O、Docker 和 Podman 等容器管理工具将可以像使用 runc 和 Clear Container/runV （Kata） 那样使用这个新的底层运行时。（目前我的团队没有人参与这方面的工作。）<br>
总结如下，在 3-4 年前，上游开发者打算编写一个底层的 golang 工具用于启动容器，最终这个工具就是 runc。那时开发者有一个使用 C 编写的 lxc 工具，在 runc 开发后，他们很快转向 runc。我很确信，当决定构建 libcontainer 时，他们对 systemd-nspawn 或其它非原生（即不使用 golang）的运行 namespaces 隔离的容器的方式都不感兴趣。<br>
via: https://opensource.com/article/18/1/history-low-level-container-runtimes</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[《openshift 4 在企业中的实践》读书笔记]]></title>
        <id>https://frankpig.github.io/post/lesslessopenshift-4-zai-qi-ye-zhong-de-shi-jian-greatergreater-du-shu-bi-ji/</id>
        <link href="https://frankpig.github.io/post/lesslessopenshift-4-zai-qi-ye-zhong-de-shi-jian-greatergreater-du-shu-bi-ji/">
        </link>
        <updated>2022-09-16T06:48:56.000Z</updated>
        <summary type="html"><![CDATA[<!-- more -->
<p>ocp4的基础架构和运维</p>
]]></summary>
        <content type="html"><![CDATA[<!-- more -->
<p>ocp4的基础架构和运维</p>
<!-- more -->
<p>[TOC]</p>
<h1 id="红帽与docker">红帽与Docker</h1>
<p>红帽开发CRI-O，Docker开发Containerd。</p>
<h1 id="调用关系">调用关系</h1>
<p>从概念上，从PaaS顶层到底层的调用关系是：<br>
Orchestration API <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Container Engine API <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Kernel API</p>
<h1 id="ocp3的调用架构">ocp3的调用架构：</h1>
<p>Kubernetes Master <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Kubelet <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Docker Engine <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Containerd <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> runC <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Linux Kernel</p>
<h1 id="ocp4的调用架构">ocp4的调用架构：</h1>
<p>Kubernetes Master <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Kubelet <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> CRI-O <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> runC <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span></span></span></span> Linux Kernel</p>
<ul>
<li>CRI-O的运行不依赖于守护进程，即使ocp节点上的CRI-O的Systemd进程终止，所有在此节点上运行的pod也不受影响。</li>
</ul>
<h1 id="coreos">coreOS</h1>
<p>CoreOS系统，rpm-ostree 文件系统布局如下：</p>
<ol>
<li>/usr 是操作系统二进制文件和库的存储位置，只读。</li>
<li>/etc, /boot, /var在系统上可写，只能通过machine config operator更改。</li>
<li>/var/lib/containers 适用于存储容器镜像的graph存储位置。</li>
</ol>
<ul>
<li>新的coreos部署在升级过程中进行，并在下次重启时生效；如果升级出现问题，则进行一次回滚并重启。升级在ocp集群更新期间执行，不能单独升级coreos。</li>
</ul>
<h1 id="operator">Operator</h1>
<h2 id="cluster-operator">Cluster Operator</h2>
<ul>
<li>每个co相关的namespace通常有两个，一个负责运行openrator本身的pod，另一个负责运行由这个operator控制的、负责具体工作的pod。</li>
<li>e.g. dns通过Operator的启动和控制流程为：<br>
i. dns Cluster Operator创建dns-default Daemonset<br>
ii. dns-default Daemonset在每个ocp节点创建coredns pod，负责集群中的dns解析<br>
iii. 通过配置dns cluster operator的参数来改变coredns的行为</li>
</ul>
<h2 id="application-operator">Application Operator</h2>
<ul>
<li>在operatorHub里面发布的operator</li>
<li>operator framework<br>
i. operator sdk<br>
ii. operator metering：为提供专业服务的operator启用使用情况报告<br>
iii. operator lifecycle manager：提供了一种陈述式的方式来安装、管理和升级operator以及operator在集群中所依赖的资源。</li>
</ul>
<h1 id="ocp核心进程简介">ocp核心进程简介</h1>
<h2 id="master">Master</h2>
<ul>
<li>kube-apiserver<br>
i. 验证并配置api对象的数据<br>
ii. 提供rest操作服务<br>
iii. 为集群的共享状态提供前端访问</li>
<li>etcd<br>
i. 存放kube-apiserver的相关数据</li>
<li>controller manager<br>
i. 只在master运行<br>
ii. 监控对象包括node, workload, namespace, service account</li>
<li>scheduler</li>
</ul>
<h2 id="node">Node</h2>
<ul>
<li>kubelet<br>
i. node上运行的node agent<br>
ii. 可以通过hostname将node注册到kube-apiserver</li>
<li>kube-proxy<br>
i. 实现kubernetes service的通信与负载均衡机制的重要组件<br>
ii. 负责为pod创建代理服务<br>
iii. 从kube-apiserver获取所有service信息，并根据service信息创建代理服务，实现service到pod的请求路由和转发</li>
<li>容器运行时</li>
</ul>
<h1 id="ocp核心进程分析">ocp核心进程分析</h1>
<h2 id="master-2">Master</h2>
<ul>
<li>进程的运行方式<br>
i. 普通pod（cluster operator管理）<br>
ii. 静态pod<br>
iii. systemd服务<br>
b. 静态pod不需要连接到api server即可启动，由特定节点上的kubelet守护程序直接管理<br>
i. 登录master，切换到/etc/kubernetes/static-pod-resources/<br>
ii. ls看到etcd, kube-controller-manager, kube-apiserver, kube-scheduler<br>
iii. 以上pod在ocp安装完成前，在bootstrap阶段已经启动<br>
c. systemd服务<br>
i. kubelet<br>
ii. crio<br>
d. 普通pod<br>
i. vsphere-csi-controller<br>
ii. csi-snapshot-controller<br>
iii. machine-config-controller<br>
iv. multus-admission-controller<br>
v. sdn-controller<br>
2. Node<br>
a. kube-proxy<br>
i. 安装方式取决于集群配置的sdn插件<br>
ii. 可以通过network operator的参数来配置kube-proxy<br>
1) oc edit networks.operator.openshift.io<br>
iii. 配置参数<br>
1) iptablesSyncPeriod：iptables规则的刷新时间间隔<br>
2) bindAddress：kube-proxy绑定的地址<br>
3) proxyArguments：传递给kube-proxy额外的命令行参数<br>
iv. iptables的同步触发条件：<br>
1) events触发：是指service或endpoint对象发生变化<br>
2) 超过kube-proxy设定的同步时间<br>
ocp的认证与授权
<ol>
<li>认证<br>
a. 认证方式<br>
i. oauth access token<br>
1) 可以通过用户登录、serviceaccount或者api获取<br>
ii. x509 client certificate<br>
1) 用于集群组件向api server认证<br>
b. user<br>
i. ocp中与api server进行交互的实体<br>
ii. 分为system user/ regular user/ service account<br>
c. identity<br>
i. 在使用identity providers中的用户登录后，会自动在ocp中创建user对象和identity对象，identity对象中记录user和identity providers的信息，实现唯一标识一个用户。<br>
d. 过程<br>
i. user尝试向api server进行身份验证<br>
ii. oauth server通过identity providers验证请求者的身份<br>
iii. 通过验证后，oauth server向用户提供oauth access token</li>
<li>授权<br>
a. group<br>
b. role<br>
i. cluster role<br>
ii. local role</li>
</ol>
</li>
</ul>
<h1 id="ocp的scc安全上下文约束">ocp的scc（安全上下文约束）</h1>
<pre><code>1. 运行特权容器
2. 在容器中获取额外的能力
3. 使用宿主机目录作为pod volume
4. 容器的selinux上下文
5. 容器的uid
6. 宿主机命名空间和网络的使用
7. pod volume的属组fsGroup
8. 附加组的配置
9. 根文件系统只读
10. 可以使用的volume类型
11. 配置允许的seccomp策略

scc通过setting和strategy两部分来控制pod的权限，这些配置的值分为以下三类：
1. 通过布尔值设置，只有True/False
2. 通过以组允许的值设置，比如Allow Volume Types
3. 通过策略控制
	a. run as user
		i. 控制pod启动后运行进程的用户
	b. selinux context
		i. 控制pod启动后的selinux标签
	c. supplemental groups
		i. 控制pod运行用户的附加组
	d. fsgroup
</code></pre>
<p>优先级顺序：pod定义&gt; scc策略 &gt; project annotations默认值</p>
<p>SCC匹配<br>
1. 过滤<br>
a. pod有权限使用scc列表<br>
2. 匹配<br>
a. 逐个匹配pod的需求与各项策略的定义。<br>
b. 如果有多个scc满足，则需要评估scc的优先级。<br>
c. 优先级相同，则选择策略限制更加严格的scc。</p>
<p>ocp中pod调度策略<br>
默认调度策略<br>
1. predicates<br>
a. static 静态<br>
i. 不接受用户的任何配置参数或输入<br>
b. general 普通<br>
i. non-critical predicates——针对非关键pod<br>
ii. essential predicates——针对所有pod<br>
2. priorities<br>
a. static 静态<br>
i. 除权重外不接受用户的任何配置参数。<br>
ii. 必须要指定权重。<br>
b. configurable 可配置</p>
<p>高级调度策略<br>
1. node selectors<br>
2. affinity/anti-affinity<br>
3. taints/tolerations</p>
<p>pod调度程序算法<br>
1. 过滤节点<br>
a. pod定义资源请求<br>
b. 评估节点列表是否有污点<br>
2. 对节点候选短列表排优先级<br>
a. 算出候选列表中各个节点的加权得分<br>
3. 选择最适合的节点</p>
<p>ocp的网络介绍与规划<br>
ocp内部通信网络整体概述<br>
1. ocp中pod之间的通信是否一定需要service？<br>
a. 不，可以直接在pod里面curl其他pod的ip:port<br>
2. 从一个pod访问另外一个pod，pod之间可以通信，其数据链路是什么？<br>
a. 两个pod如果在一个节点上，其通信流量没有绕出本宿主机的ovs<br>
b. 如果两个pod在不同节点上，pod之间的通信经过了vxlan<br>
3. 既然pod之间通信不需要service，为何k8s引入service？<br>
a. 负载均衡：service提供其所属多个pod的负载均衡。<br>
b. 服务注册与发现：解决不同服务之间的通信问题。<br>
i. 每创建一个service，就会分配一个ip，称为clusterIp，这样内部nds就可以通过service名称（FQDN）解析成clusterIp地址，这样就完成了服务注册，dns解析需要的信息全部保存在etcd中。<br>
ii. etcd中记录的service注册信息有namespace、service name、clusterIp，这三个信息可以组成dns A记录（通过查询生成）<br>
4. 有了service之后，pod之间的通信和没有service有何区别？<br>
a. 在数据通信层，没区别，因为service只是逻辑层面的东西。<br>
b. 但是没有service，无法实现负载均衡，需要第三方实现。<br>
5. clusterIp到pod IP这部分的负载均衡是如何实现的？<br>
a. kube-proxy（iptables模式）</p>
<p>ocp的网络模型<br>
1. pod内部容器通信的网络<br>
a. k8通过为pod分配统一的网络空间，实现了多个容器之间的网络共享，也就是同一个pod中的容器之间通过localhost相互通信。<br>
2. pod与pod通信的网络<br>
a. cni规范<br>
b. 基于二层实现<br>
i. 通过将pod放在一个大二层网络中，跨节点通信通常使用vxlan或udp封包实现，常用的此类插件有flannel、ovs等。<br>
c. 基于三层实现<br>
i. 将pod放在一个互联互通的网络中，通常使用路由实现，常用的此类插件有calico、flannel-gw等。<br>
d. ocp默认使用的是基于ovs的二层网络。<br>
3. pod和service通信的网络<br>
a. 既然pod之间通信不需要service，为何k8s引入service？<br>
b. k8提供了三种负载均衡方式：userspace，iptables, ipvs<br>
c. ocp默认使用iptables，有两个缺点<br>
i. 不支持复杂的负载均衡算法<br>
ii. 当选择的某个后端pod没有响应时无法自动重新连接到另一个pod，需要利用pod的健康检查来确保endpoints列表中的pod都是存活的<br>
4. 集群外部与service或pod通信的网络<br>
a. hostport<br>
b. nodeport<br>
c. hostNetwork<br>
d. loadBalancer<br>
e. ingress/router</p>
<p>ocp pod网络的实现<br>
openshift SDN<br>
• ocp中pod网络默认用ocp sdn实现和维护<br>
• 底层是使用OpenvSwitch实现的二层覆盖网络，跨节点通信使用Vxlan封包<br>
• 提供三种模式<br>
○ subnet：提供扁平的pod网络。<br>
§ 集群中每个pod都可以与其他pod（本项目或其他项目）进行通信<br>
○ multitenant：提供项目级别的隔离。<br>
§ 除default项目外，默认所有项目之间隔离。<br>
§ 每个项目会收到唯一的虚拟网络id（vnid），用于标识分配给项目的pod流量。<br>
§ 不同vnid项目中的pod之间无法互相通信，除了vnid为0的项目default。<br>
○ networkpolicy：ocp默认的ovs插件模式，提供pod粒度级别的隔离<br>
§ 模式的隔离完全由networkpolicy对象控制，项目管理员可以创建网络策略。<br>
§ 支持按namespace/pod级别进行网络访问控制。<br>
§ ocp sdn默认使用了networkpolicy模式，初始情况下，所有项目中没有任何的networkpolicy对象。<br>
§ 底层使用iptables，所以策略不宜作用域大量独立pod<br>
• pod访问外部网络，通过snat，最终以pod所在的worker节点ip访问外部网络。<br>
• 实现pod访问外部网络的控制，主要有两种方式<br>
○ 配置namespace级别的egress ip<br>
§ 通过为namespace指定egress ip，并将egress ip分配到指定的节点实现；支持自动/手动配置。<br>
○ 配置egress防火墙<br>
§ 在集群中创建EgressNetworkPolicy对象。<br>
§ 每个项目中只能定义一个EgressNetworkPolicy对象。<br>
§ 支持multitenant/networkpolicy<br>
• 通过multus-cni可以实现pod多网络平面，让一个pod同时配置多个网卡连接到多个网络。</p>
<p>ocp中dns的实现<br>
• ocp使用coreDNS，提供ocp内部的域名解析服务。<br>
• coreDNS会监听k8 api，当新创建一个service时，coreDNS中就会提供<service-name>.<project-name>.svc.cluster.local域名的解析；还可以通过<service-name>.<project-name>.endpoints.cluster.local解析endpoints<br>
○ 来自同一项目的pod可以直接使用service作为短域名<br>
○ 来自不同项目的pod可以使用service名称和项目名称作为短域名<br>
• 每个节点会启动一个coreDNS容器，在kubelet将--cluster-dns设定为coreDNS的service clusterIP，这样pod就可以使用coreDNS进行域名解析。<br>
• cluster domain定义了集群中pod和service域名的基本dns域，默认为cluster.local<br>
• dns解析过程<br>
a. 宿主机上应用的dns解析直接通过宿主机上/etc/resolv.conf中配置的上游dns服务器解析；同时这也表明在宿主机上默认无法解析k8s的service域名<br>
b. pod中的应用直接通过pod中配置的dns server解析所有域名，该域名会将解析查询分配到具体的coreDNS实例中。<br>
c. 在coreDNS实例中，如果有cache缓存，则直接返回；如果没有cache，则判断，若解析域名属于cluster.local, in-addr.arpa或ip6.arpa， 则通过coreDNS的k8插件去查询，本质上是通过k8 api查询etcd中保存的数据实现域名解析ip地址的返回，否则转到宿主机/etc/resolv.conf中配置的上游dns服务器。</p>
<p>ocp上ocn-kubernetes的实现<br>
• 整体上看，ovn在实现overlay, service, NAT方面，效率和性能高于sdn<br>
• ovn模式中，ocp将不再需要kube-proxy，因此也就不再需要iptables实现。</p>
<p>ocp外部访问的实现<br>
• hostport<br>
○ 在宿主机上运行的容器，为了外部能够访问这个容器，将容器的端口与宿主机进行端口映射。<br>
○ 优点是易操作。<br>
○ 缺点是无法支持复杂业务场景，并且容器间的相互访问比较困难。<br>
• nodeport<br>
○ 是service的一种类型。<br>
○ 本质上是通过在集群的每个节点上暴露一个端口，然后将这个端口映射到service的端口。<br>
○ nodeport和hostport最重要的区别是hostport是针对一个单一宿主机的一个容器，而nodeport是针对k8集群而言的。<br>
○ 缺点很明显，宿主机端口浪费和安全隐患，并且数据转发次数较多。<br>
• hostnetwork<br>
○ hostnetwork是pod运行的一种模式。<br>
○ pod的ip和端口会直接绑定到宿主机的ip和端口。<br>
○ 在数据中心的ocp，router就是以hostnetwork模式运行的（公有云环境中router通过loadbalancer类型service对外暴露）<br>
○ 相比于nodeport，优势在于可以直接使用宿主机网络，转发路径短，性能好。<br>
○ 缺点是占用节点的实际端口，无法在一个节点同时运行相同端口的两个pod。<br>
• loadBalancer<br>
○ 也是service的一种类型，用于和云平台负载均衡器结合。<br>
○ 实际上是通过向底层云平台申请创建一个负载均衡器来向外暴露服务。<br>
• ingress/router<br>
○ 本质上ingress就是用于配置域名转发，并实时监控应用pod的变化，动态的更新负载均衡的配置文件。<br>
○ ingress包含两大组件，ingress controller和ingress。<br>
§ ingress是一种资源对象，声明域名和service对应的问题。<br>
§ ingress controller是负载均衡器，加载ingress动态生成负载均衡配置。<br>
○ ocp默认的router本质上是一个以hostnetwork方式运行在节点上的容器化HAproxy，可提供http、https、websockets、tls with sni协议的访问。<br>
§ router相当于ingress controller<br>
§ route相当于ingress对象<br>
§ ocp中可以同时使用router或ingress对象对外暴露服务。<br>
• 外部访问方式使用建议<br>
○ 对于http、https类的七层应用，往往通过router暴露fqdn的方式访问。<br>
○ 对于非http、https类的四层应用（如mysql），存在两种情况：<br>
§ 单个节点运行一个副本：hostnetwork（转发路径短，性能更好）<br>
§ 单个节点运行多个副本：nodeport</p>
<p>ocp四层ingress实现<br>
1. 如果ingress大多数是七层，采用默认的router即可。<br>
2. 如果有少量的四层ingress需求，采用默认的HAproxy+nodeport。<br>
3. HAproxy默认支持七层，如果要支持四层，需要定制模板。<br>
4. ocp上的nginx ingress operator（loadBalancer模式）可以实现四层ingress，但前端需要能够提供loadBalancer service ip的硬件负载均衡器。此外，还需要通过全局的（nginx-ingress命名空间）configmap实现，还需要手工写要暴露的应用的service全名。<br>
5. 如果ocp部署在裸机上，又不想引入f5，可以使用MetalLB为nginx ingress controller提供ip。</p>
<p>ocp的存储介绍与规划<br>
pv和pvc<br>
• pv是一个开放的存储管理框架。<br>
○ ocp支持nfs, glusterFS, cinder, ceph, EBS, iSCSI, fibre channel<br>
• pvc是用户的一个volume请求。<br>
• pv只有被pvc绑定后才能被pod挂载使用。<br>
• pv和pvc绑定的生命周期<br>
○ available<br>
§ pv创建完成，处于可用状态。<br>
○ pending<br>
§ pv和pvc处于匹配状态。<br>
§ 匹配策略有访问模式和卷大小以及label匹配。<br>
§ 如果无法匹配，则pvc会一直处于pending状态。<br>
○ bound<br>
§ pv和pvc已经绑定，这个状态的pvc才能被pod挂载使用。<br>
○ released<br>
§ 挂载pvc的pod被删除，pvc处于释放状态。<br>
○ failure<br>
§ 删除pvc，pv转变为回收状态，该状态下的pv无法直接被新的pvc绑定。<br>
§ 回收状态下pv是否保留数据取决于pv的回收策略定义，默认会保留。<br>
§ 如果想要将该状态的pv转变为available，必须删除pv然后重新创建。<br>
• pv有三种访问模式<br>
a. readWriteOnce<br>
§ pv可以由单个pod以读写方式挂载<br>
b. readOnlyMany<br>
§ pv可以由多个pod以只读方式挂载<br>
c. readWriteMany<br>
§ pv可以由多个pod以读写方式挂载<br>
• 动态pv优先，如果动态pv无法满足pvc需求，才会匹配静态pv。</p>
<p>ocp存储容量规划<br>
• etcd<br>
○ 保存在master节点的本地磁盘，默认在/var/lib/etcd<br>
○ 20GB以上<br>
• docker/cri-o本地存储<br>
○ 100GB以上<br>
• ocp节点本地日志存储<br>
○ 默认存放在/var/log<br>
○ 15GB以上<br>
• 镜像仓库<br>
○ 推荐对象存储&gt;文件系统存储&gt;块存储<br>
• 日志系统<br>
○ 推荐块存储&gt;文件系统存储<br>
• 监控系统<br>
○ 推荐块存储&gt;文件系统存储</p>
<p>ocp高可用<br>
1. 控制节点<br>
a. 存储层<br>
§ 采用raft<br>
§ follower, candidate, leader<br>
§ follower在一段时间内没有收到来自leader的心跳，就会转变为candidate<br>
b. 管理层<br>
§ controller-manager服务<br>
§ 同一时刻只允许多个节点的一个服务处理任务（一主多从）<br>
§ 利用etcd强一致性实现选举<br>
§ 节点初始化时，controller-manager会向etcd注册leader<br>
c. 接入层<br>
§ apiserver服务<br>
§ apiserver本身无状态，可以实现多活。<br>
§ 通常采用在apiserver前端加负载均衡实现。<br>
d. 如果故障节点大于一半以上，集群就会进入只读模式。<br>
2. router<br>
a. 建议使用hostnetwork<br>
b. 在多个节点上运行多个router<br>
c. 前端通过软件/硬件负载均衡到多个router<br>
3. 镜像仓库<br>
a. 默认都是使用docker-distribution，属于无状态应用。<br>
4. 管理控制台<br>
a. 指的是console ui界面<br>
b. 启动多个容器（因为是无状态应用）</p>
]]></content>
    </entry>
</feed>